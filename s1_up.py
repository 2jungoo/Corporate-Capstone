"""
Step 1: AI ê¸°ë°˜ ë¼ì§€ ì„±ì¥ íŒ¨í„´ í•™ìŠµ ì‹œìŠ¤í…œ
- Random Forestì™€ XGBoostë¥¼ í™œìš©í•œ ì„±ì¥ íŒ¨í„´ ë¶„ì„
- ì²´ì¤‘ êµ¬ê°„ë³„ ì¦ì²´ìœ¨ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
- Feature Engineering ë° ëª¨ë¸ í‰ê°€
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import os
import chardet
from scipy import stats as scipy_stats
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import joblib
import warnings

warnings.filterwarnings('ignore')


def detect_encoding(file_path):
    """íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)
            result = chardet.detect(raw_data)
            return result['encoding']
    except:
        return None


def read_csv_with_encoding(file_path):
    """ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì‹œë„í•˜ì—¬ CSV íŒŒì¼ ì½ê¸°"""
    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin1', 'iso-8859-1']

    try:
        detected_encoding = detect_encoding(file_path)
        if detected_encoding:
            try:
                df = pd.read_csv(file_path, encoding=detected_encoding)
                return df, detected_encoding
            except:
                pass
    except:
        pass

    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            return df, encoding
        except:
            continue

    return None, None


class AIGrowthPatternLearner:
    """AI ê¸°ë°˜ ì„±ì¥ íŒ¨í„´ í•™ìŠµ"""

    def __init__(self, base_path='./í…ìŠ¤íŠ¸ ë°ì´í„°'):
        self.base_path = base_path
        self.all_data = {}
        self.growth_patterns = {}
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}

        # ê¸°ë³¸ íŒ¨í„´
        self.default_patterns = self.get_default_patterns()

    def get_default_patterns(self):
        """ì—…ê³„ í‘œì¤€ ê¸°ë³¸ ì„±ì¥ íŒ¨í„´"""
        return {
            'overall': {
                'mean_daily_gain': 0.65,
                'std_daily_gain': 0.15,
                'median_daily_gain': 0.63,
                'min_weight': 20.0,
                'max_weight': 110.0,
                'mean_weight': 50.0,
                'mean_feed': 1.8,
                'std_feed': 0.5
            },
            'weight_bins': {
                '0-20kg': {'mean_daily_gain': 0.45, 'std_daily_gain': 0.10},
                '20-40kg': {'mean_daily_gain': 0.65, 'std_daily_gain': 0.12},
                '40-60kg': {'mean_daily_gain': 0.75, 'std_daily_gain': 0.10},
                '60-80kg': {'mean_daily_gain': 0.80, 'std_daily_gain': 0.10},
                '80-100kg': {'mean_daily_gain': 0.70, 'std_daily_gain': 0.12},
                '100kg+': {'mean_daily_gain': 0.55, 'std_daily_gain': 0.15}
            }
        }

    def load_all_data(self):
        """ëª¨ë“  chamberì˜ ë°ì´í„° ë¡œë“œ"""
        print("\n" + "="*80)
        print("ğŸ“š Step 1: ê¸°ì¡´ ë°ì´í„° ë¡œë”© ì¤‘...")
        print("="*80)

        data_found = False

        for chamber in ['chamber1', 'chamber2', 'chamber3', 'chamber4']:
            self.all_data[chamber] = {}
            print(f"\n[{chamber}] ë°ì´í„° ë¡œë”©...")

            # ì²´ì¤‘ ë°ì´í„°
            weight_path = os.path.join(self.base_path, chamber, 'ë¼ì§€ì²´ì¤‘')
            if os.path.exists(weight_path):
                csv_files = list(Path(weight_path).glob('*.csv'))
                weight_data = []
                for csv_file in csv_files:
                    df, encoding = read_csv_with_encoding(str(csv_file))
                    if df is not None:
                        df['chamber'] = chamber
                        weight_data.append(df)

                if weight_data:
                    self.all_data[chamber]['weight'] = pd.concat(weight_data, ignore_index=True)
                    print(f"  âœ“ ì²´ì¤‘: {len(self.all_data[chamber]['weight'])}ê±´")
                    data_found = True

            # ì‚¬ë£Œ ë°ì´í„°
            feed_path = os.path.join(self.base_path, chamber, 'ì‚¬ì–‘ê´€ë¦¬', 'ì„­ì·¨ëŸ‰')
            if os.path.exists(feed_path):
                csv_files = list(Path(feed_path).glob('*.csv'))
                feed_data = []
                for csv_file in csv_files:
                    df, encoding = read_csv_with_encoding(str(csv_file))
                    if df is not None:
                        df['chamber'] = chamber
                        feed_data.append(df)

                if feed_data:
                    self.all_data[chamber]['feed'] = pd.concat(feed_data, ignore_index=True)
                    print(f"  âœ“ ì‚¬ë£Œ: {len(self.all_data[chamber]['feed'])}ê±´")
                    data_found = True

        if not data_found:
            print("\nâš ï¸ ì‹¤ì œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
            self.generate_sample_data()

        return self.all_data

    def generate_sample_data(self):
        """í•™ìŠµìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        print("\nğŸ“Š í•™ìŠµìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")

        n_pigs = 100
        days = 150

        sample_data = []

        for pig_id in range(n_pigs):
            start_weight = np.random.uniform(20, 30)
            current_weight = start_weight

            for day in range(days):
                # ì²´ì¤‘ êµ¬ê°„ë³„ ì„±ì¥ë¥ 
                if current_weight < 40:
                    base_gain = 0.65
                elif current_weight < 60:
                    base_gain = 0.75
                elif current_weight < 80:
                    base_gain = 0.80
                elif current_weight < 100:
                    base_gain = 0.70
                else:
                    base_gain = 0.55

                # ì¼ë ¹ íš¨ê³¼
                age_factor = 1.0
                if day < 30:
                    age_factor = 0.7 + (day / 30) * 0.3
                elif day > 120:
                    age_factor = max(0.6, 1.0 - (day - 120) / 200)

                daily_gain = base_gain * age_factor + np.random.normal(0, 0.1)
                daily_gain = np.clip(daily_gain, 0.2, 1.2)

                current_weight += daily_gain

                # ì‚¬ë£Œ ì„­ì·¨ëŸ‰
                feed = current_weight * 0.035 + np.random.normal(0, 0.2)
                feed = max(0.5, feed)

                sample_data.append({
                    'pig_id': pig_id,
                    'day': day,
                    'weight': current_weight,
                    'daily_gain': daily_gain,
                    'feed': feed,
                    'chamber': f'chamber{(pig_id % 4) + 1}'
                })

        self.sample_df = pd.DataFrame(sample_data)
        print(f"âœ“ {len(sample_data)}ê±´ì˜ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ")

    def create_features(self, df):
        """ë¨¸ì‹ ëŸ¬ë‹ feature ìƒì„±"""
        df = df.copy()

        # ê¸°ë³¸ íŠ¹ì„±
        df['weight_bin'] = pd.cut(df['weight'],
                                   bins=[0, 20, 40, 60, 80, 100, 200],
                                   labels=['0-20', '20-40', '40-60', '60-80', '80-100', '100+'])

        # Lag features (ì´ì „ ì‹œì  ë°ì´í„°)
        df = df.sort_values(['pig_id', 'day'])
        df['weight_lag1'] = df.groupby('pig_id')['weight'].shift(1)
        df['weight_lag3'] = df.groupby('pig_id')['weight'].shift(3)
        df['weight_lag7'] = df.groupby('pig_id')['weight'].shift(7)

        # Rolling features (ì´ë™ í‰ê· )
        df['weight_rolling_mean_7'] = df.groupby('pig_id')['weight'].transform(
            lambda x: x.rolling(window=7, min_periods=1).mean()
        )
        df['weight_rolling_std_7'] = df.groupby('pig_id')['weight'].transform(
            lambda x: x.rolling(window=7, min_periods=1).std()
        )

        # ì„±ì¥ ì†ë„ íŠ¹ì„±
        df['weight_change_1d'] = df['weight'] - df['weight_lag1']
        df['weight_change_3d'] = df['weight'] - df['weight_lag3']
        df['weight_change_7d'] = df['weight'] - df['weight_lag7']

        # ì²´ì¤‘ ëŒ€ë¹„ ì‚¬ë£Œ ë¹„ìœ¨
        df['feed_weight_ratio'] = df['feed'] / df['weight']

        # ëˆ„ì  ì¼ìˆ˜ íŠ¹ì„±
        df['day_squared'] = df['day'] ** 2
        df['day_cubed'] = df['day'] ** 3

        # ì²´ì¤‘ ì œê³±
        df['weight_squared'] = df['weight'] ** 2

        return df

    def train_random_forest_model(self, df):
        """Random Forest ëª¨ë¸ í•™ìŠµ"""
        print("\nğŸŒ² Random Forest ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # Feature ìƒì„±
        df_features = self.create_features(df)
        df_features = df_features.dropna()

        # Feature ì„ íƒ
        feature_cols = [
            'weight', 'day', 'feed',
            'weight_lag1', 'weight_lag3', 'weight_lag7',
            'weight_rolling_mean_7', 'weight_rolling_std_7',
            'weight_change_1d', 'weight_change_3d', 'weight_change_7d',
            'feed_weight_ratio', 'day_squared', 'weight_squared'
        ]

        X = df_features[feature_cols]
        y = df_features['daily_gain']

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Random Forest ëª¨ë¸
        rf_model = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )

        rf_model.fit(X_train_scaled, y_train)

        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = rf_model.predict(X_test_scaled)

        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"\nğŸ“Š Random Forest ì„±ëŠ¥:")
        print(f"   - RMSE: {rmse:.4f} kg/day")
        print(f"   - MAE: {mae:.4f} kg/day")
        print(f"   - RÂ² Score: {r2:.4f}")

        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nğŸ” ì£¼ìš” Feature (Top 5):")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")

        # ëª¨ë¸ ì €ì¥
        self.models['random_forest'] = rf_model
        self.scalers['random_forest'] = scaler
        self.feature_importance['random_forest'] = feature_importance

        return rf_model, scaler, feature_importance

    def train_xgboost_model(self, df):
        """XGBoost ëª¨ë¸ í•™ìŠµ"""
        print("\nğŸš€ XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # Feature ìƒì„±
        df_features = self.create_features(df)
        df_features = df_features.dropna()

        feature_cols = [
            'weight', 'day', 'feed',
            'weight_lag1', 'weight_lag3', 'weight_lag7',
            'weight_rolling_mean_7', 'weight_rolling_std_7',
            'weight_change_1d', 'weight_change_3d', 'weight_change_7d',
            'feed_weight_ratio', 'day_squared', 'weight_squared'
        ]

        X = df_features[feature_cols]
        y = df_features['daily_gain']

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # XGBoost ëª¨ë¸
        xgb_model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=7,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )

        xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )

        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = xgb_model.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"\nğŸ“Š XGBoost ì„±ëŠ¥:")
        print(f"   - RMSE: {rmse:.4f} kg/day")
        print(f"   - MAE: {mae:.4f} kg/day")
        print(f"   - RÂ² Score: {r2:.4f}")

        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': xgb_model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nğŸ” ì£¼ìš” Feature (Top 5):")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")

        # ëª¨ë¸ ì €ì¥
        self.models['xgboost'] = xgb_model
        self.feature_importance['xgboost'] = feature_importance

        return xgb_model, feature_importance

    def extract_patterns_from_models(self, df):
        """ëª¨ë¸ì—ì„œ íŒ¨í„´ ì¶”ì¶œ"""
        print("\nğŸ“ˆ ì„±ì¥ íŒ¨í„´ ì¶”ì¶œ ì¤‘...")

        df_features = self.create_features(df)
        df_features = df_features.dropna()

        # ì²´ì¤‘ êµ¬ê°„ë³„ í†µê³„
        weight_bins_patterns = {}

        for bin_name in ['0-20', '20-40', '40-60', '60-80', '80-100', '100+']:
            bin_data = df_features[df_features['weight_bin'] == bin_name]

            if len(bin_data) > 0:
                weight_bins_patterns[f'{bin_name}kg'] = {
                    'mean_daily_gain': float(bin_data['daily_gain'].mean()),
                    'std_daily_gain': float(bin_data['daily_gain'].std()),
                    'median_daily_gain': float(bin_data['daily_gain'].median()),
                    'min_daily_gain': float(bin_data['daily_gain'].quantile(0.1)),
                    'max_daily_gain': float(bin_data['daily_gain'].quantile(0.9)),
                    'count': int(len(bin_data)),
                    'mean_feed': float(bin_data['feed'].mean()),
                    'std_feed': float(bin_data['feed'].std())
                }

        # ì „ì²´ í†µê³„
        overall_patterns = {
            'mean_daily_gain': float(df_features['daily_gain'].mean()),
            'std_daily_gain': float(df_features['daily_gain'].std()),
            'median_daily_gain': float(df_features['daily_gain'].median()),
            'min_weight': float(df_features['weight'].min()),
            'max_weight': float(df_features['weight'].max()),
            'mean_weight': float(df_features['weight'].mean()),
            'mean_feed': float(df_features['feed'].mean()),
            'std_feed': float(df_features['feed'].std())
        }

        self.growth_patterns = {
            'overall': overall_patterns,
            'weight_bins': weight_bins_patterns,
            'model_type': 'ai_based',
            'models_trained': list(self.models.keys())
        }

        return self.growth_patterns

    def visualize_model_performance(self, df, output_path='./ai_model_performance.png'):
        """ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”"""
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™” ì¤‘...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Feature importance ë¹„êµ
        ax1 = axes[0, 0]
        if 'random_forest' in self.feature_importance and 'xgboost' in self.feature_importance:
            rf_importance = self.feature_importance['random_forest'].head(10)
            xgb_importance = self.feature_importance['xgboost'].head(10)

            x = np.arange(len(rf_importance))
            width = 0.35

            ax1.barh(x - width/2, rf_importance['importance'], width, label='Random Forest', alpha=0.7)
            ax1.barh(x + width/2, xgb_importance['importance'], width, label='XGBoost', alpha=0.7)
            ax1.set_yticks(x)
            ax1.set_yticklabels(rf_importance['feature'])
            ax1.set_xlabel('Feature Importance')
            ax1.set_title('Feature Importance Comparison', fontweight='bold')
            ax1.legend()
            ax1.grid(True, alpha=0.3, axis='x')

        # ì²´ì¤‘ êµ¬ê°„ë³„ ì¦ì²´ìœ¨
        ax2 = axes[0, 1]
        if 'weight_bins' in self.growth_patterns:
            bins = list(self.growth_patterns['weight_bins'].keys())
            means = [self.growth_patterns['weight_bins'][b]['mean_daily_gain'] for b in bins]
            stds = [self.growth_patterns['weight_bins'][b]['std_daily_gain'] for b in bins]

            ax2.bar(range(len(bins)), means, yerr=stds, alpha=0.7, capsize=5)
            ax2.set_xticks(range(len(bins)))
            ax2.set_xticklabels(bins, rotation=45)
            ax2.set_ylabel('Daily Gain (kg/day)')
            ax2.set_title('Weight Bin Growth Rates', fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='y')

        # ì„±ì¥ ê³¡ì„ 
        ax3 = axes[1, 0]
        df_plot = self.sample_df if hasattr(self, 'sample_df') else df
        if df_plot is not None and len(df_plot) > 0:
            growth_curve = df_plot.groupby('day')['weight'].agg(['mean', 'std']).reset_index()

            ax3.plot(growth_curve['day'], growth_curve['mean'], 'b-', linewidth=2, label='Mean')
            ax3.fill_between(growth_curve['day'],
                            growth_curve['mean'] - growth_curve['std'],
                            growth_curve['mean'] + growth_curve['std'],
                            alpha=0.3, label='Â±1 Std Dev')
            ax3.axhline(y=80, color='red', linestyle='--', label='Target (80kg)')
            ax3.set_xlabel('Day')
            ax3.set_ylabel('Weight (kg)')
            ax3.set_title('Growth Curve with Uncertainty', fontweight='bold')
            ax3.legend()
            ax3.grid(True, alpha=0.3)

        # ì¦ì²´ìœ¨ ë¶„í¬
        ax4 = axes[1, 1]
        if df_plot is not None and len(df_plot) > 0 and 'daily_gain' in df_plot.columns:
            ax4.hist(df_plot['daily_gain'], bins=50, alpha=0.7, edgecolor='black', density=True)
            ax4.axvline(x=df_plot['daily_gain'].mean(), color='red', linestyle='--',
                       linewidth=2, label=f"Mean: {df_plot['daily_gain'].mean():.3f}")
            ax4.axvline(x=df_plot['daily_gain'].median(), color='orange', linestyle='--',
                       linewidth=2, label=f"Median: {df_plot['daily_gain'].median():.3f}")
            ax4.set_xlabel('Daily Gain (kg/day)')
            ax4.set_ylabel('Density')
            ax4.set_title('Daily Gain Distribution', fontweight='bold')
            ax4.legend()
            ax4.grid(True, alpha=0.3, axis='y')

        plt.suptitle('AI Model Performance Analysis', fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ ê·¸ë˜í”„ ì €ì¥: {output_path}")
        plt.close()

    def save_models(self, output_dir='./models'):
        """ëª¨ë¸ê³¼ íŒ¨í„´ ì €ì¥"""
        print("\nğŸ’¾ ëª¨ë¸ ë° íŒ¨í„´ ì €ì¥ ì¤‘...")

        os.makedirs(output_dir, exist_ok=True)

        # ëª¨ë¸ ì €ì¥
        for model_name, model in self.models.items():
            model_path = os.path.join(output_dir, f'{model_name}_model.pkl')
            joblib.dump(model, model_path)
            print(f"   âœ“ {model_name} ëª¨ë¸ ì €ì¥: {model_path}")

        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        for scaler_name, scaler in self.scalers.items():
            scaler_path = os.path.join(output_dir, f'{scaler_name}_scaler.pkl')
            joblib.dump(scaler, scaler_path)
            print(f"   âœ“ {scaler_name} ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")

        # íŒ¨í„´ ì €ì¥
        pattern_path = os.path.join(output_dir, 'growth_patterns_ai.json')
        with open(pattern_path, 'w', encoding='utf-8') as f:
            json.dump(self.growth_patterns, f, indent=2, ensure_ascii=False)
        print(f"   âœ“ ì„±ì¥ íŒ¨í„´ ì €ì¥: {pattern_path}")

        # Feature importance ì €ì¥
        for model_name, importance_df in self.feature_importance.items():
            importance_path = os.path.join(output_dir, f'{model_name}_importance.csv')
            importance_df.to_csv(importance_path, index=False, encoding='utf-8-sig')
            print(f"   âœ“ {model_name} Feature Importance ì €ì¥: {importance_path}")

    def run_learning(self):
        """ì „ì²´ AI í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        print("="*80)
        print("ğŸ“ Step 1: AI ê¸°ë°˜ ì„±ì¥ íŒ¨í„´ í•™ìŠµ ì‹œì‘")
        print("="*80)

        # 1. ë°ì´í„° ë¡œë“œ
        self.load_all_data()

        # 2. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        if hasattr(self, 'sample_df'):
            df = self.sample_df
        else:
            # ì‹¤ì œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í†µí•©
            df = None
            # ì—¬ê¸°ì— ì‹¤ì œ ë°ì´í„° í†µí•© ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

        if df is None or len(df) == 0:
            print("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
            self.generate_sample_data()
            df = self.sample_df

        # 3. AI ëª¨ë¸ í•™ìŠµ
        self.train_random_forest_model(df)
        self.train_xgboost_model(df)

        # 4. íŒ¨í„´ ì¶”ì¶œ
        self.extract_patterns_from_models(df)

        # 5. ì‹œê°í™”
        self.visualize_model_performance(df)

        # 6. ëª¨ë¸ ì €ì¥
        self.save_models()

        print("\n" + "="*80)
        print("âœ… Step 1 ì™„ë£Œ!")
        print("="*80)
        print(f"\nìƒì„±ëœ íŒŒì¼:")
        print(f"  1. models/ - í•™ìŠµëœ AI ëª¨ë¸ë“¤")
        print(f"  2. growth_patterns_ai.json - AI ê¸°ë°˜ ì„±ì¥ íŒ¨í„´")
        print(f"  3. ai_model_performance.png - ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸ· ë¼ì§€ ì¶œí•˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Step 1: AI ê¸°ë°˜ í•™ìŠµ")
    print("="*80)

    base_path = './í…ìŠ¤íŠ¸ ë°ì´í„°'

    learner = AIGrowthPatternLearner(base_path=base_path)
    learner.run_learning()

    print("\n" + "="*80)
    print("ğŸ‰ AI ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  Step 2 ì‹¤í–‰ â†’ AI ëª¨ë¸ë¡œ ì‹ ê·œ ë¼ì§€ ì˜ˆì¸¡")


if __name__ == "__main__":
    main()